steps: 5_000
warmup_steps: 2_000
name: "unnamed"
output: "outputs/cross_tokenizer_distill"
num_workers: 16
log_interval: 10
sync_interval: 100
eval_interval: 5000
save_interval: 5000
losses: [alm_unbiased]
target_tokenizer_name: google/gemma-2-2b-it:source=Gemma2
tokens_to_add: [<|start_header_id|>,<|end_header_id|>,<|eot_id|>,<|eom_id|>,<|python_tag|>]

train_model_mode: "lora"
model_lora_rank: 64
model_lora_alpha: 64
train_embeddings: true

bce_temp: 100.0
alm_diff_fn: "binary_ce"
alm_mode: "append_space"
tokenizer_pair_data_path: "artifacts/tokenizer_data/gemma2_to_qwen2"
tokenizer_pair_bias_threshold: 1.e-4
tokenizer_pair_bias_threshold_side_path: null

student:
  pretrained_model_name_or_path: "benjamin/gemma-2-2b-it-flax"
  tokenizer_name: "google/gemma-2-2b-it:source=Gemma2"

teacher:
  pretrained_model_name_or_path: "benjamin/OpenMath2-Llama3.1-8B-flax"
  tokenizer_name: "nvidia/OpenMath2-Llama3.1-8B:source=Llama3"

data:
  batch_size: 16
  kind: "hf_saved"
  lang_code: en
  dataset_configs:
    - path: "/lfs/openmathinstruct2"

hypernet:
  architecture: transformer
  num_layers: 1
  residual: true
  residual_alpha: 1
  use_attention: false

optimizer:
  type: adamw
  weight_decay: 0.01
  b1: 0.9
  b2: 0.95
  eps: 1.e-8
  grad_acc_steps: null
  learning_rate: 1.e-5
  max_grad_norm: 1.0
  param_groups:
    - pattern: .*(projector_query|projector_s2t|projector_t2s|projector_latents|loss_weights).*
      lr_scale: 2

eval:
  tasks: [arc_easy,arc_challenge,piqa,hellaswag,boolq,arithmetic,mmlu]
  lengths: [128, 256, 512, 1024, 2048]
  tokens_per_batch: 8192
  add_bos: true
  chat_template_mode: surround_instruct
  confirm_run_unsafe_code: true