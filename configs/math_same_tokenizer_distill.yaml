steps: 5_000
warmup_steps: 2_000
name: "unnamed"
output: "outputs/same_tokenizer_distill"
num_workers: 16
log_interval: 10
sync_interval: 100
eval_interval: 5000
save_interval: 5000
losses: [alm_unbiased]
target_tokenizer_name: meta-llama/Llama-3.2-3B-Instruct:source=Llama3

train_model_mode: "lora"
model_lora_rank: 64
model_lora_alpha: 64
train_embeddings: true

bce_temp: 100.0
alm_diff_fn: "binary_ce"
alm_mode: "space_merge+append_space"
tokenizer_pair_data_path: "artifacts/tokenizer_data/math_llama3_to_llama3"
tokenizer_pair_bias_threshold: 0.1
tokenizer_pair_bias_threshold_side_path: null

student:
  pretrained_model_name_or_path: "benjamin/Llama-3.2-3B-Instruct-flax"
  tokenizer_name: "meta-llama/Llama-3.2-3B-Instruct:source=Llama3"

teacher:
  pretrained_model_name_or_path: "benjamin/OpenMath2-Llama3.1-8B-flax"
  tokenizer_name: "nvidia/OpenMath2-Llama3.1-8B:source=Llama3"

data:
  batch_size: 16
  num_workers: 16
  kind: "hf"
  mix_languages: false
  streaming: false
  dataset_configs:
    - lang_code: en
      kwargs:
        path: benjamin/OpenMathInstruct-2-2M-formatted
        split: train

hypernet:
  architecture: transformer
  num_layers: 1
  residual: true
  residual_alpha: 1
  use_attention: false

optimizer:
  type: adamw
  weight_decay: 0.01
  b1: 0.9
  b2: 0.95
  eps: 1.e-8
  grad_acc_steps: null
  learning_rate: 1.e-5
  max_grad_norm: 1.0
  param_groups:
    - pattern: .*(projector_query|projector_s2t|projector_t2s|projector_latents|loss_weights).*
      lr_scale: 2

eval:
  tasks: [arc_easy,arc_challenge,piqa,hellaswag,boolq,arithmetic,mmlu]
  lengths: [128, 256, 512, 1024, 2048]
  tokens_per_batch: 8192
  add_bos: true
  chat_template_mode: surround_instruct
  confirm_run_unsafe_code: true